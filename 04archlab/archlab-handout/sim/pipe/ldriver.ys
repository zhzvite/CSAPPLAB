#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Describe how and why you modified the baseline code.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	iaddq $-10,%rdx
	jl L0R9
Loop10:
		mrmovq (%rdi),%r8
                mrmovq 0x8(%rdi),%r9
                mrmovq 0x10(%rdi),%r10
                mrmovq 0x18(%rdi),%r11
                mrmovq 0x20(%rdi),%r12
                mrmovq 0x28(%rdi),%r13
                mrmovq 0x30(%rdi),%r14
                mrmovq 0x38(%rdi),%rcx
                mrmovq 0x40(%rdi),%rbx
                mrmovq 0x48(%rdi),%rbp

		rmmovq %r8,(%rsi)
                rmmovq %r9,0x8(%rsi)
                rmmovq %r10,0x10(%rsi)
                rmmovq %r11,0x18(%rsi)
                rmmovq %r12,0x20(%rsi)
                rmmovq %r13,0x28(%rsi)
                rmmovq %r14,0x30(%rsi)
                rmmovq %rcx,0x38(%rsi)
                rmmovq %rbx,0x40(%rsi)
                rmmovq %rbp,0x48(%rsi)

		andq %r8,%r8
		jle judge0
		iaddq $1,%rax
judge0:
		
		
		andq %r9,%r9
		jle judge1
		iaddq $1,%rax
judge1:

		
		andq %r10,%r10
		jle judge2
		iaddq $1,%rax
judge2:
		
		
		andq %r11,%r11
		jle judge3
		iaddq $1,%rax
judge3:
		
		
		andq %r12,%r12
		jle judge4
		iaddq $1,%rax
judge4:
		
		
		andq %r13,%r13
		jle judge5
		iaddq $1,%rax
judge5:
		
		
		andq %r14,%r14
		jle judge6
		iaddq $1,%rax
judge6:
		
		
		andq %rcx,%rcx
		jle judge7
		iaddq $1,%rax
judge7:
		
		
		andq %rbx,%rbx
		jle judge8
		iaddq $1,%rax
judge8:				
		andq %rbp,%rbp
		jle step
		iaddq $1,%rax
step:
		iaddq $0x50,%rdi
		iaddq $0x50,%rsi
		iaddq $-10,%rdx
		jge Loop10
# applying range checks to remainders
L0R9:
        iaddq   $7,%rdx         # Compare with 3 (len + 10 - 3)
        jl      L0R2            # len < 3
        jg      L4R9            # len > 3
        je      Rem3            # len == 3
L0R2:
        iaddq   $2,%rdx         # Compare with 1 (len + 3 - 1)
        je      Rem1            # len == 1
        jg      Rem2            # len == 2
        ret                     # len == 0
L4R6:
        iaddq   $2,%rdx         # Compare with 5 (len + 7 - 5)
        jl      Rem4            # len == 4
        je      Rem5            # len == 5
        jg      Rem6            # len == 6
L4R9:
        iaddq   $-4,%rdx        # Compare with 7 (len + 3 - 7)
        jl      L4R6            # len < 7
        je      Rem7            # len == 7
L8R9:
        iaddq   $-1,%rdx        # Compare with 8 (len + 7 - 8)
        je      Rem8            # len == 8

# dealing with remainders
Rem9:
        mrmovq 0x40(%rdi), %r8
        rmmovq %r8, 0x40(%rsi)
        andq %r8, %r8
        jle Rem8
        iaddq $1, %rax
Rem8:
        mrmovq 0x38(%rdi), %r8
        rmmovq %r8, 0x38(%rsi)
        andq %r8, %r8
        jle Rem7
        iaddq $1, %rax
Rem7:
        mrmovq 0x30(%rdi), %r8
        rmmovq %r8, 0x30(%rsi)
        andq %r8, %r8
        jle Rem6
        iaddq $1, %rax
Rem6:
        mrmovq 0x28(%rdi), %r8
        rmmovq %r8, 0x28(%rsi)
        andq %r8, %r8
        jle Rem5
        iaddq $1, %rax
Rem5:
        mrmovq 0x20(%rdi), %r8
        rmmovq %r8, 0x20(%rsi)
        andq %r8, %r8
        jle Rem4
        iaddq $1, %rax
Rem4:
        mrmovq 0x18(%rdi), %r8
        rmmovq %r8, 0x18(%rsi)
        andq %r8, %r8
        jle Rem3
        iaddq $1, %rax
Rem3:
        mrmovq 0x10(%rdi), %r8
        rmmovq %r8, 0x10(%rsi)
        andq %r8, %r8
        jle Rem2
        iaddq $1, %rax
Rem2:
        mrmovq 0x8(%rdi), %r8
        rmmovq %r8, 0x8(%rsi)
        andq %r8, %r8
        jle Rem1
        iaddq $1, %rax
Rem1:
        mrmovq (%rdi), %r8
        rmmovq %r8, (%rsi)
        andq %r8, %r8
        jle Done
        iaddq $1, %rax

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad -2
	.quad -3
	.quad 4
	.quad 5
	.quad 6
	.quad 7
	.quad 8
	.quad 9
	.quad 10
	.quad 11
	.quad -12
	.quad -13
	.quad 14
	.quad -15
	.quad 16
	.quad 17
	.quad -18
	.quad -19
	.quad 20
	.quad -21
	.quad 22
	.quad -23
	.quad -24
	.quad -25
	.quad -26
	.quad -27
	.quad -28
	.quad -29
	.quad -30
	.quad 31
	.quad -32
	.quad 33
	.quad -34
	.quad -35
	.quad 36
	.quad -37
	.quad -38
	.quad -39
	.quad 40
	.quad 41
	.quad -42
	.quad 43
	.quad -44
	.quad 45
	.quad -46
	.quad -47
	.quad -48
	.quad 49
	.quad -50
	.quad -51
	.quad 52
	.quad 53
	.quad 54
	.quad 55
	.quad 56
	.quad -57
	.quad -58
	.quad -59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
